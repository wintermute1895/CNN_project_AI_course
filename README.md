成员：迟旭、刘竞聪、张津亮、李安琪、高鸣邦

**一、具身智能在这个项目中的体现 

“具身智能 (Embodied AI)” 的核心在于智能体 (Agent) 能够通过自身的“身体” (Embodiment，如机器人、灵巧手) 与环境进行交互，通过感知环境来做出决策和行动，并从交互中学习。

1.  **感知-行动循环 (Perception-Action Loop) 的基础框架：**
    *   **感知 (Perception)：**
        *   **核心体现：** 使用CNN (YOLOv5) 处理D405相机（或模拟的图像/视频输入源）的视觉信息，**识别和定位**环境中的关键元素（即我们定义的障碍物）。
        *   **代码体现：** 成员D负责的检测脚本 (`detect_image.py`, `detect_video.py`) 就是感知模块的核心。它接收视觉输入，输出对环境的理解（障碍物的位置、类别）。
    *   **决策/规划 (Decision Making/Planning - 简化版)：**
        *   **核心体现：** 基于感知模块输出的障碍物信息，系统需要做出一个简单的“决策”。在一周内，这个决策可能非常简单，比如判断障碍物是否在关键区域、是否需要避让。
        *   **代码体现：** 成员D或成员A可以在检测脚本的输出部分，加入简单的逻辑判断。例如：
            ```python
            # 在检测到物体后
            for *xyxy, conf, cls in detections:
                object_name = model.names[int(cls)]
                if object_name == "dangerous_obstacle" and conf > 0.7:
                    # 假设 xyxy 定义了边界框
                    # 这里可以加入一个简单的判断，比如障碍物是否在图像中心区域
                    # if is_obstacle_critical(xyxy, image_width, image_height):
                    print(f"警告：检测到关键障碍物 {object_name}！建议采取避让措施。")
                    # 这里是“行动”的起点
            ```
    *   **行动 (Action - 模拟或极简版)：**
        *   **核心体现：** 根据决策，系统需要输出一个“动作指令”。在一周内，这个动作指令可能不是直接控制物理硬件，而是：
            *   **打印文本指令：** 如上述代码中的`print`语句。
            *   **改变屏幕上的可视化：** 比如在检测到障碍物时，在图像上用红色高亮显示，或显示一个“STOP”图标。
            *   **(如果时间允许且有现成简单接口) 调用一个预设的硬件动作：** 比如，如果LEAP HAND有非常简单的API可以调用预设的姿态（如“停止并张开手掌”），可以在检测到特定障碍物时触发这个API调用。
        *   **代码体现：** 同样在检测脚本的后续逻辑中。

2.  **环境交互的意图：**
    *   虽然我们可能无法实现复杂的物理交互，但项目的目标是**为了与环境交互**而进行感知。我们检测障碍物是为了让灵巧手能够“知道”环境并做出“响应”（即使响应是模拟的）。这种面向交互的感知是具身智能的关键。

3.  **模型在特定“身体”（D405相机视角）和特定环境下的应用：**
    *   我们训练YOLOv5所使用的数据集（无论是公开的还是自定义的）都是从一个特定的“视角”（模拟D405的视角）观察到的特定“工作环境”。模型学习到的特征和检测能力是与这个“身体”和环境相关的。

**在一周内，具身智能的体现更侧重于“感知环境以准备行动”的这个前半环，并将“行动”部分极大简化。** 关键是建立起从视觉输入到初步决策（或指令建议）的流程。

**二、怎么控制硬件

1.  **优先级最低，作为“锦上添花”：** 首先确保CNN的障碍物检测算法能在PC上稳定运行并达到预期效果。
2.  **寻找最简单的现有接口和示例：**
    *   **D405相机：**
        *   **目标：** 能否快速获取到图像帧并在Python中显示？
        *   **方法：** 查找Intel RealSense SDK的Python示例代码，看是否有简单的“打开相机并显示图像”的脚本。如果这个过程超过1-2小时还无法跑通，果断放弃或降级。
        *   **代码贡献 (可分配给D或A)：** 编写一个独立的 `capture_D405_image.py` 脚本，如果成功，可以将其实时帧作为检测脚本的输入源。
    *   **LEAP HAND灵巧手：**
        *   **目标：** 能否通过Python脚本调用1-2个**预设的、最简单的**动作？（例如：完全张开、完全握拳、或者一个“停止”姿态）。
        *   **方法：** 查找LEAP HAND的SDK或API文档，看是否有极其简单的Python示例可以直接调用几个固定的动作ID或状态。**不要尝试复杂的关节控制或轨迹规划。**
        *   **代码贡献 (可分配给A或有硬件经验的同学)：** 编写一个独立的 `control_leap_hand_simple.py` 脚本，包含几个函数如 `leap_hand_stop()`, `leap_hand_open()`。
3.  **如果直接硬件控制困难，采用模拟方式：**
    *   **模拟D405输入：** 使用预先录制好的视频文件，或者一个图片文件夹作为检测脚本的输入源，模拟相机正在“看到”的场景。这是最稳妥的方式。
    *   **模拟LEAP HAND响应：**
        *   **文本输出：** 如前所述，在检测到障碍物时，打印出“灵巧手执行避障动作：向左移动”或“灵巧手停止”。
        *   **可视化模拟：** 如果有时间，可以用Pygame或OpenCV画一个简单的框代表灵巧手，在检测到障碍物时，让这个框改变颜色或位置。这需要额外的编码工作。

4.  **“开环”控制优先于“闭环”控制：**
    *   **开环：** CNN检测 -> 判断 -> (如果硬件能动) 执行一个预设动作。动作执行后不根据新的视觉反馈调整。
    *   **闭环：** CNN检测 -> 判断 -> 执行动作 -> 再次CNN检测 -> 根据新的环境状态调整动作... 这个在一周内基本不可能。

**结论：硬件控制在一周内，目标应该是“能让它动一下，证明概念可行”，而不是追求稳定、精确、复杂的控制。如果获取图像或调用预设动作非常复杂，果断放弃，专注于PC上的算法演示。**

**三、每个人在分工里都要写代码 (确保代码贡献的分布)**

**成员A (协调与算法)：**

- **CNN结构层面 (核心)：**
    
    - **主导并实现至少一个明确的“CNN结构改进”尝试。** 这可以是：
        
        1. **集成一个轻量级注意力模块：** 选择一个如SEBlock或简单的通道/空间注意力，编写其PyTorch模块代码 (可AI辅助生成骨架)，然后在models/common.py中定义，并修改YOLOv5的.yaml配置文件（例如yolov5s_custom.yaml）在Backbone或Neck的特定位置插入该模块。**这是最推荐的，因为代码量适中，又能体现结构修改。**
            
        2. **(如果时间更紧) 调整现有模块的参数或组合：** 例如，在.yaml中尝试不同的C3模块重复次数，或者调整不同卷积层的out_channels，分析其对模型大小和性能的影响。这虽然改动小，但也算调整CNN结构参数。
            
    - **代码：** models/common.py (新模块), models/yolov5s_custom.yaml (新配置)。
        
    - **解释：** 解释为什么选择这个注意力模块/结构调整，它的原理是什么，如何集成到YOLOv5中，以及期望它带来什么效果。
        
- **训练参数/策略层面 (辅助)：**
    
    - 指导C进行超参数调优，例如分析TensorBoard后建议调整学习率策略或数据增强参数。
        
    - **代码：** 可能涉及修改hyp.scratch-*.yaml文件或train.py的调用参数。
        
    - **解释：** 解释这些参数对训练过程的影响。
        

**成员B (数据)：**

- **CNN相关代码 (间接但重要)：**
    
    - **数据增强策略的理解与配置：**
        
        - 深入研究YOLOv5 hyp.scratch-*.yaml 文件中定义的各种数据增强方法 (Mosaic, MixUp, HSV, flip, scale, translate等)。
            
        - **编写一个Python脚本，使用albumentations库或直接调用YOLOv5数据加载器中的增强函数，可视化这些增强在自定义数据集上的实际效果。**
            
        - 根据对增强效果的理解和对自定义数据集特点的分析，**尝试调整hyp.scratch-*.yaml中的增强参数**，并与A和C讨论，作为一组实验进行对比。
            
    - **代码：** visualize_augmentations.py, 修改后的hyp.scratch-custom.yaml。
        
    - **解释：** 解释各种数据增强的原理，它们如何帮助CNN模型提高泛化能力，以及为什么选择调整特定的增强参数。
        
    - **(可选，如果涉及RGB-D) 编写将深度图转换为适合CNN输入的伪彩色图或额外通道的预处理脚本。**
        

**成员C (模型训练与评估)：**

- **CNN相关代码 (主要在实验执行和分析层面)：**
    
    - **执行A设计的CNN结构改进实验：** 使用A提供的yolov5s_custom.yaml和修改后的代码进行训练。
        
    - **执行B建议的数据增强策略实验：** 使用B调整后的hyp.scratch-custom.yaml进行训练。
        
    - **编写脚本对比不同CNN变体/训练策略的性能：** 例如，加载多个不同实验训练出的.pt模型，在同一个测试集上运行评估，并将结果（mAP、Precision, Recall, F1-score等）汇总到表格或绘制对比图。
        
    - **代码：** compare_model_performance.py，以及管理多组训练的shell/python脚本。
        
    - **解释：** 如何通过实验验证CNN结构修改或训练策略调整的有效性，如何解读和对比各项性能指标。
        
- **训练参数/策略层面 (核心)：**
    
    - 负责系统地调整核心训练超参数（学习率、优化器、batch size、epochs），进行多组实验，寻找最优组合。
        
    - **代码：** 主要是train.py的命令行参数组合，以及记录这些实验的脚本。
        
    - **解释：** 解释各个超参数的意义，它们如何影响训练，以及调参的策略和观察到的现象。
        

**成员D (检测脚本与应用)：**

- **CNN相关代码 (主要在模型加载和输出解析)：**
    
    - **理解YOLOv5模型的输出张量：** 研究models/yolo.py中Detect模块的输出格式，以及utils/general.py中non_max_suppression函数如何处理这些输出来得到最终的边界框、置信度和类别。
        
    - **在自己的检测脚本中正确加载和使用不同版本的CNN模型：** 确保能加载A或C训练出的不同.pt文件（包括基线模型和改进模型）。
        
    - **(如果A的改进涉及到Head的微小调整) 需要在检测脚本中适配这种输出变化。**
        
    - **代码：** detect_*.py中模型加载、输入预处理、输出后处理部分。
        
    - **解释：** YOLOv5检测头的输出是什么含义，NMS的作用是什么，如何将模型的原始输出转换为可视化的检测结果。
        

**成员E (文档、演示与测试)：**

- **CNN相关代码 (测试和理解层面)：**
    
    - **编写简单的单元测试或集成测试脚本，验证A、B、C、D编写的与CNN相关的核心函数或模块是否按预期工作。** 例如：
        
        - 测试A实现的注意力模块，给定一个伪输入，看输出维度是否正确。
            
        - 测试B的数据增强可视化脚本是否能正常生成图片。
            
        - 测试D的检测脚本加载模型和处理单张图片的功能。
            
    - **代码：** test_attention_module.py, test_data_augmentation_viz.py, test_detector_basic.py。
        
    - **解释：** 测试的重要性，如何为CNN相关代码编写简单的测试用例，以及从团队成员处收集并整理他们对各自CNN相关代码的解释，写入最终报告的“方法”和“个人贡献”部分。
        

**通过这样的细化：**

- **成员A** 负责最核心的CNN结构设计与实现决策，以及可能的编码。
    
- **成员B** 的工作虽然围绕数据，但通过数据增强策略的调整和可视化脚本的编写，直接与CNN的训练输入和泛化能力相关。
    
- **成员C** 通过执行不同CNN变体和训练策略的实验，并编写对比分析脚本，深入理解CNN性能。
    
- **成员D** 在应用层面与CNN的输入输出打交道，理解模型如何被使用。
    
- **成员E** 通过编写测试脚本和整理文档，确保对团队的CNN工作有整体的把握和质量控制。

---
好的，我们来把这个一周冲刺计划落实到更具体的YOLOv5代码修改、个人编写内容、协作流程以及先后顺序上。**核心目标是在YOLOv5的框架内，让每个人都有机会修改或编写与CNN直接相关的代码，并理解其作用。**

**前提假设：**

*   团队已有一个共享的GitHub仓库，YOLOv5作为外部依赖被克隆到项目的一个子目录（例如`yolov5/`）并被主项目的`.gitignore`忽略，或者团队决定在一个YOLOv5的fork上工作（这种情况下，修改可以直接在fork的YOLOv5代码中进行，但要注意保持分支清晰）。**为简单起见，我们假设修改发生在YOLOv5代码的本地副本中，并且会将关键的修改或新增文件提交到你们自己的主项目仓库。**
*   主要开发分支是 `develop`，最终稳定版本合并到 `main`。每个人从 `develop` 创建特性分支。
*   AI辅助编码被允许并鼓励，但核心理解和解释由成员负责。

**一、具体每个人需要修改YOLOv5的哪些代码 / 需要写什么？**

**成员A (协调与算法核心)：**

*   **主要负责的YOLOv5代码修改/编写：**
    1.  **模型配置文件 (`.yaml`) 的设计与修改 (核心CNN搭建体现)：**
        *   **文件：** 复制一份官方的 `yolov5/models/yolov5s.yaml` (或其他尺寸) 并重命名为例如 `yolov5_custom.yaml` (这个文件放在你们自己项目的配置目录下，或者如果直接改YOLOv5的fork，就在那里改)。
        *   **修改内容：**
            *   **集成新模块：** 如果团队决定加入注意力机制或其他自定义模块，A负责在`.yaml`的`backbone`或`neck`部分正确地插入这些模块的调用，并配置其参数（如通道数）。
            *   **调整结构参数：** 尝试调整现有模块的深度（如`C3`模块的`number`参数）或宽度（如`Conv`模块的`out_channels`参数），以探索不同模型容量。
            *   **代码示例 (yaml部分)：**
                ```yaml
                # In yolov5_custom.yaml
                backbone:
                  # ... original layers ...
                  [[-1, 3, C3, [128]], # Original C3
                   [-1, 1, SEBlock, [128, 16]], # <-- A's new SEBlock integration (args: out_channels, reduction_ratio)
                   [-1, 1, Conv, [256, 3, 2]],
                  # ... rest of the backbone ...
                ```
    2.  **(可选，如果新模块复杂) `yolov5/models/common.py` 的新模块定义：**
        *   **文件：** `yolov5/models/common.py` (或者将新模块定义在你们自己项目的一个`custom_modules.py`中，然后在`.yaml`里正确引用)。
        *   **编写内容：** 实现团队决定加入的新的CNN基础模块，例如`SEBlock`, `CBAM`, 或其他自定义卷积块。
        *   **代码示例 (python部分)：**
            ```python
            # In common.py or custom_modules.py
            class SEBlock(nn.Module):
                def __init__(self, c1, r=16): # c1: input channels, r: reduction ratio
                    super().__init__()
                    self.squeeze = nn.AdaptiveAvgPool2d(1)
                    self.excitation = nn.Sequential(
                        nn.Conv2d(c1, c1 // r, 1), # Use Conv2d for flexibility
                        nn.SiLU(),
                        nn.Conv2d(c1 // r, c1, 1),
                        nn.Sigmoid()
                    )
                def forward(self, x):
                    return x * self.excitation(self.squeeze(x))
            ```
    3.  **训练脚本 (`yolov5/train.py`) 的调用参数优化与实验设计：**
        *   虽然不直接修改`train.py`源码，但A负责设计实验，决定使用哪些`--cfg` (指向`yolov5_custom.yaml`), `--hyp` (指向自定义的超参数文件), `--weights` 等参数组合来训练和对比不同的CNN结构或训练策略。
    4.  **(可选) `yolov5/utils/loss.py` 的微调：**
        *   如果决定尝试调整损失函数的不同组成部分的权重，或者引入一个非常微小的损失函数修改。

*   **个人编写的独立脚本：**
    *   `run_experiments.py` (或shell脚本): 用于批量、可重复地运行不同配置的训练任务。
    *   `analyze_model_complexity.py`: 脚本加载不同的`.pt`或`.yaml`定义的模型，分析其参数量(FLOPs)。

**成员B (数据总管)：**

*   **主要负责的YOLOv5代码修改/编写：**
    1.  **数据增强超参数文件 (`yolov5/data/hyps/hyp.scratch-*.yaml`) 的理解与定制：**
        *   **文件：** 复制一份官方的`hyp.scratch-low.yaml` (或medium/high) 并重命名为 `hyp.scratch_custom.yaml`。
        *   **修改内容：** 深入理解其中各项数据增强参数（如`hsv_h`, `hsv_s`, `hsv_v`, `degrees`, `translate`, `scale`, `shear`, `perspective`, `flipud`, `fliplr`, `mosaic`, `mixup`, `copy_paste`）的含义。根据对自定义数据集的观察和AI的建议，**有针对性地调整这些参数的值或开启/关闭某些增强**，以期提升模型在特定场景下的泛化能力。
    *   **代码示例 (yaml部分)：**
        ```yaml
        # In hyp.scratch_custom.yaml
        degrees: 10.0  # 调整旋转角度范围
        translate: 0.15 # 调整平移范围
        scale: 0.6     # 调整缩放范围
        mixup: 0.1     # 调整MixUp的alpha参数
        # ... 其他参数 ...
        ```

*   **个人编写的独立脚本：**
    *   `data_formatter.py`: 将公开数据集 (如COCO子集) 转换为YOLO TXT格式的脚本。
    *   `split_dataset.py`: 将整理好的数据集划分为训练集、验证集、测试集的脚本。
    *   `analyze_dataset.py`: 统计数据集中类别分布、标注框大小/长宽比分布的脚本，输出可视化图表。
    *   `visualize_augmentations.py`: **加载YOLOv5的数据加载器 (`LoadImagesAndLabels` from `utils/dataloaders.py`)，并可视化经过不同数据增强参数处理后的样本图像**，以直观理解增强效果。

**成员C (模型训练与评估工程师)：**

*   **主要负责的YOLOv5代码修改/编写：**
    1.  **(可选，如果A的改进涉及到损失函数) 协助A修改 `yolov5/utils/loss.py`：**
        *   例如，调整`ComputeLoss`类中不同损失分量（`box_loss`, `obj_loss`, `cls_loss`）的平衡权重。
    2.  **`yolov5/val.py` 的理解与定制化输出 (如果需要)：**
        *   深入理解`val.py`如何计算mAP等指标。
        *   如果需要输出更详细的每类别AP，或者特定的错误分析，可能需要微调`val.py`的绘图或日志记录部分。

*   **个人编写的独立脚本：**
    *   `training_manager.sh` (或 `.py`): 编写脚本，使用不同的 `--cfg`, `--hyp`, `--epochs`, `--batch-size` 等参数组合，系统地调用 `yolov5/train.py` 来执行多组对比实验。
    *   `plot_training_curves.py`: 编写脚本解析 `runs/train/exp*/results.csv` 或 TensorBoard 日志，使用Matplotlib/Seaborn绘制更美观或更具对比性的训练曲线 (如不同实验的mAP vs epochs)。
    *   `evaluate_models_detailed.py`: 编写脚本加载多个训练好的 `.pt` 模型，在测试集上批量运行评估，并将所有关键指标（mAP@0.5, mAP@0.5:0.95, Precision, Recall, F1 per class）汇总到一张表格或生成对比报告。

**成员D (检测脚本与应用工程师)：**

*   **主要负责的YOLOv5代码修改/编写：**
    1.  **`yolov5/utils/general.py` 中 `non_max_suppression` 函数的理解：**
        *   这个函数是检测结果后处理的核心。D需要理解其参数（如`conf_thres`, `iou_thres`, `classes`, `agnostic_nms`）如何影响最终的检测输出。
    2.  **`yolov5/utils/plots.py` 中绘图函数的理解与定制 (如果需要)：**
        *   理解 `plot_one_box` 等函数是如何在图像上绘制边界框和标签的。
        *   如果需要自定义绘制样式（如不同颜色的框代表不同置信度，或者添加额外的文本信息），可以复制并修改这些函数。

*   **个人编写的独立脚本 (核心贡献)：**
    *   `detect_image_custom.py`: 加载训练好的`.pt`模型，接收单张图片作为输入，进行障碍物检测，并在图片上清晰地绘制边界框、类别标签、置信度，最后保存或显示结果图片。**这个脚本需要D从头编写，但可以大量参考 `yolov5/detect.py` 的逻辑。**
    *   `detect_video_custom.py`: 类似上一个脚本，但输入是视频文件或摄像头ID，逐帧进行检测并显示/保存。
    *   `process_detections.py`: (可选) 一个模块，包含D编写的函数，用于进一步处理`detect_*.py`脚本得到的原始检测结果列表（例如，根据障碍物位置或大小进行筛选，或者实现一个简单的基于规则的响应逻辑）。

**成员E (文档、演示与测试工程师)：**

*   **主要负责的YOLOv5代码修改/编写：**
    1.  **(可选，如果团队决定) 参与 `yolov5/export.py` 的使用和理解：**
        *   学习如何将训练好的`.pt`模型转换为ONNX或其他格式，这可能有助于后续的（模拟）部署或性能分析。
    2.  **为其他成员编写的CNN相关模块或核心脚本编写测试用例。**

*   **个人编写的独立脚本：**
    *   `test_custom_module.py`: (配合A或C) 如果A或C实现了新的CNN模块 (如SEBlock)，E可以编写一个简单的单元测试脚本，输入伪数据，检查模块的输出形状和基本功能。
    *   `test_detection_script.py`: (配合D) 编写一个简单的集成测试脚本，使用几张固定的测试图片输入到D的`detect_image_custom.py`，然后检查输出图片是否生成，或者（如果可能）输出的检测结果文本是否包含预期的内容（这比较难做精确断言，但至少保证脚本能跑通）。
    *   `generate_report_assets.py`: (可选) 编写脚本自动从训练日志、评估结果中提取数据，生成用于报告或PPT的图表和表格。

**二、整个协作流程是怎样的？需要谁先干什么？后干什么？**

**Day 0 (准备日 - A可以提前开始)：**
*   **A：** 创建GitHub仓库，初始化`main`和`develop`分支，设置分支保护规则。克隆YOLOv5官方仓库到本地作为参考。确定一个基础的YOLOv5版本（如v5s）。

**Day 1-2: 项目启动、数据与环境并行**
*   **并行任务：**
    *   **A：** 熟悉YOLOv5代码结构，设计初步的CNN改进方案（如决定加入SEBlock），搭建自己的核心开发环境。在`develop`分支上创建特性分支 `feature/A/initial_setup_and_cnn_plan`。
    *   **B：** 开始调研公开数据集，编写数据格式转换脚本的初稿。在`develop`分支上创建特性分支 `feature/B/dataset_preprocessing_scripts`。
    *   **C：** 搭建训练环境，跑通YOLOv5官方示例训练，熟悉`train.py`和`val.py`的用法。在`develop`分支上创建特性分支 `feature/C/training_env_setup`。
    *   **D：** 搭建检测环境，跑通`detect.py`，开始构思自己的检测脚本。在`develop`分支上创建特性分支 `feature/D/detection_script_design`。
    *   **E：** 熟悉Git协作流程，创建报告和PPT框架，开始写`README.md`。在`develop`分支上创建特性分支 `feature/E/docs_and_ppt_setup`。
*   **依赖关系：** B的数据处理脚本需要尽快提供给C进行训练。
*   **每日结束：** 每个人将自己的特性分支推送到远程，并向`develop`分支发起PR（即使未完成，也可以是Draft PR，用于代码分享和早期反馈）。A负责审查和合并（如果成熟）。

**Day 3-4: 数据就绪，首次训练，检测脚本开发**
*   **先后顺序与并行：**
    1.  **B (先)：** 完成第一批数据集的转换、合并、配置文件创建。将相关脚本和配置文件合并到`develop`。
    2.  **C (基于B的成果)：** 使用B准备好的数据集和A（可能已经合并的）`yolov5_custom.yaml`（如果A已经做了初步修改）开始**首次模型训练**。持续监控和记录。
    3.  **A (并行)：** 如果决定添加新CNN模块（如SEBlock），开始在`common.py`（或自定义文件）中编码实现，并更新`yolov5_custom.yaml`。不断在自己的特性分支上测试。
    4.  **D (并行)：** 继续开发`detect_image_custom.py`和`detect_video_custom.py`，目标是能加载一个（可以是官方预训练的）`.pt`模型并进行检测。
    5.  **E (并行)：** 根据B的数据集描述和A/C的计划，撰写报告的“数据集”和“模型方法”部分。开始为B和D的代码编写简单的测试思路。
*   **每日结束：** 各自推送，向`develop`发起PR。A和C重点关注训练进展。

**Day 5-6: 模型评估，迭代改进，脚本完善**
*   **先后顺序与并行：**
    1.  **C (先)：** 完成第一轮（或迭代轮）训练，生成评估报告和模型权重，提交到`develop` (PR方式)。
    2.  **A (基于C的评估结果)：** 分析模型性能。如果A之前实现了新的CNN模块或结构调整，并且已经通过了初步测试，现在可以用C训练出的权重（或重新训练）来评估这个改进的效果。**这是A体现CNN改进代码的关键点。**
    3.  **D (基于C提供的模型权重)：** 将C训练好的模型加载到自己的检测脚本中，进行实际效果测试和脚本优化。
    4.  **B (并行)：** 根据A和C的反馈，看是否需要对数据增强参数 (`hyp.scratch_custom.yaml`) 进行调整，并准备一小批新的测试图片。
    5.  **E (并行)：** 撰写报告的“实验结果与分析”部分，为A/C/D的代码编写测试脚本。
*   **每日结束：** 推送，PR。重点是验证A的CNN改进是否有效。

**Day 7: 整合，最终测试，报告与演示冲刺**
*   **并行冲刺：**
    *   **A：** 最终确认用于演示的模型版本和相关配置。审查所有合并到`develop`的代码。将`develop`分支的稳定内容合并到`main`分支。
    *   **B：** 提供最终的数据集统计和样例。
    *   **C：** 确保所有训练日志和评估结果完整。
    *   **D：** 确保检测脚本在演示时能流畅运行，准备好演示素材。
    *   **E：** **核心：** 完成报告和PPT的最终整合、排版、校对。组织演示排练。
*   **最终交付：** 所有成果汇总到`main`分支并推送到GitHub。

**协作流程总结：**

1.  **每日站会：** 5-10分钟，同步进度、问题、计划。
2.  **任务拆分与认领：** 基于上述计划，每个人认领当日或阶段性的小任务。
3.  **特性分支开发：** `git checkout -b feature/...`
4.  **本地编码与提交：** `git add .`, `git commit -m "..."` (小步快跑)
5.  **推送特性分支：** `git push origin feature/...` (每日至少一次)
6.  **Pull Request (PR) 到 `develop`：** 完成一个阶段性功能或修复后，发起PR。清晰描述PR内容。
7.  **Code Review (快速)：** A或其他指定成员快速审查PR，提出修改意见或批准。
8.  **合并PR到 `develop`：** A负责合并。
9.  **同步 `develop`：** 其他成员定期 `git checkout develop`, `git pull origin develop`，然后回到自己的特性分支 `git rebase develop` (或 `git merge develop`) 以保持与主开发线同步。
10. **最终阶段：** A将稳定的 `develop` 分支合并到 `main` 分支作为最终交付。

这个流程强调了并行、小步快跑和持续集成。每个人都有明确的、与CNN相关的代码编写任务，并且通过PR和代码审查来保证协作的质量。AI辅助将大大加快每个人的编码和理解速度。