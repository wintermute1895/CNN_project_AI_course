# README

我们是来自天津大学自动化学院2025级人工智能概论课程的五人小组。本项目是我们的期末大作业。目标是在 YOLOv5s 模型的基础上，通过改进其结构来优化特定任务（如障碍物检测与运动规划）的效果，进而实现具身智能感知。旨在通过 PyBullet 仿真环境，完整展示“感知——决策——行动”的逻辑链条，以体现课程对“具身智能”的要求。

---
## 一、项目分工

**A. 迟旭（模型构建、决策逻辑脚本与统筹协调）：**

> 1.  负责基于 YOLOv5s 改进模型结构，增加了 SEBlock 模块与 CBAM 模块，并尝试修改参数和位置进行对比测试。
> 2.  编写 ONNX 可视化脚本，进行模型结构的可视化展示。
> 3.  修改 `common.py` 和 `yolo.py`（通过导入自定义模块的方式），并编写模型构建的集成测试。
> 4.  为每个新模型编写 YAML（配置文件），用于后续的训练。
> 5.  编写决策逻辑脚本，用于接收检测脚本的输入，并将输出结果传递给仿真脚本。
> 6.  每天与所有成员对接工作进展，审查代码，合并 PR，并撰写相关文档。
> 7.  编写 `.gitignore` 文件用于版本控制，写依赖文档和dockerfile

**B. 张津亮（自定义数据集搭建）：**

> 1.  查看 YOLOv5 使用的官方数据集，并从中筛选出一部分更贴近项目目标的数据用于训练模型。
> 2.  在 Kaggle 和 Hugging Face 上寻找更符合项目目标的障碍物检测型数据集。
> 3.  拍摄校园内的日常照片，并使用 LabelImg 等工具进行高质量的标注。
> 4.  将以上数据整合成为自定义数据集，并分别编写格式转换脚本，将其转换为 YOLO 能够识别的类型。
> 5.  编写数据集配置文件用于后续的训练。

**C. 李安琪（模型训练与结果可视化）：**

> 1.  使用官方模型加载预训练权重，在自定义数据集上训练出一个基线模型，后续所有的改进都将与之进行对比。
> 2.  使用自定义的数据集配置文件与自定义的模型配置文件，在随机初始化权重的条件下进行训练，从而训练出新的不同模型。
> 3.  编写脚本，将训练结果转换为可视化的对比图表，以便进行统计分析。
> 4.  实时监控训练过程，编写模型对比分析文档，并进行超参数调优。

**D. 高鸣邦（检测脚本）：**

> 1.  在 YOLOv5 官方的检测脚本基础上，编写能够获取真实图像输入、真实视频输入以及能从 PyBullet 虚拟相机获取图像输入的脚本。该脚本还能使用改进之后的模型权重文件，输出特定的检测结果。
> 2.  辅助 PPT 制作与文档编写。

**E. 刘竞聪（仿真脚本、联调脚本与PPT制作）：**

> 1.  编写 PyBullet 仿真脚本，实现对仿真模拟小车的控制以及仿真环境障碍物的搭建。
> 2.  编写联调主程序，该程序调用检测脚本、决策逻辑脚本和仿真脚本，实现封装演示仿真效果的功能。
> 3.  为各个脚本编写单元测试，部分测试集成到脚本内部。
> 4.  制作 PPT 用于汇报演示。

---
## 二、技术选型过程

经过学习和讨论，决定采用 YOLOv5s 这一成熟且轻量化的模型作为改进的基础。尝试为其增加了 SEBlock 模块和 CBAM 模块。在项目管理上，使用 Git 进行版本控制，并选择 GitHub 作为代码托管平台。为了便于其他同学或用户复现项目，考虑使用 Docker 进行封装。

---
## 三、CNN 原理回顾

首先简单回顾一下 CNN 的核心原理：CNN 通过卷积操作来提取图像特征。每个卷积核都像一个模式探测器，它有自己擅长识别的特定“模式”（如边缘、角点、纹理等）。如果输入图像的某个区域符合卷积核的“模式”，那么在卷积操作后得到的特征图中，对应区域的数值就会比较大。一张彩色图片可以看作是由 RGB 三个颜色通道叠加而成的。通过使用不同的卷积核，可以计算出多张新的特征图，这些特征图可以再次叠加形成新的“图像”，然后继续进行后续的卷积操作。这个过程会不断地将从浅层识别出来的小特征（如轮廓、直线）组合成更完整、更有意义的深层特征（如眼睛、耳朵，最终组合成猫脸）。

在这个过程中，归一化和激活函数起着辅助模型更好学习的作用。例如，激活函数就像一个“阀门”，它可以筛选掉那些特征权重不够强的结果，只保留那些比较明显的特征。这些机制都是为了提高模型学习的效率和效果。

那么，YOLOv5s 的具体结构是怎样的呢？下图是使用 ONNX 文件导出的模型结构可视化结果。从宏观角度来看，YOLOv5s 主要可以划分为三个核心部分：Backbone、Neck 和 Head。

![YOLOv5s ONNX 可视化图](此处替换为你的ONNX可视化图的路径或链接)
*(请将 `此处替换为你的ONNX可视化图的路径或链接` 替换为实际的图片链接或嵌入方式)*

### 1. Backbone (骨干网络)：从像素到特征的“炼金术”

-   **作用通俗讲：** Backbone 是模型的“眼睛”和“初级大脑”。其核心任务是将输入的原始图片（一堆像素点）转换成计算机更容易理解的“特征”。这些特征就像是图片的关键信息摘要，例如图像中哪里有边缘、哪里有角点、哪些区域的纹理比较特殊等等。
-   **YOLOv5s (v7.0) Backbone 结构解读 (结合 `yolov5s_v7_original_backbone.yaml` 或类似的标准结构进行理解)：**
    -   **初始下采样 (Stem - 例如第0层 Conv):**
        -   “一开始，模型会用一个比较大的卷积核（比如 $6 \times 6$，步长为2）对图片进行一次快速的‘粗略扫描’和‘尺寸压缩’。这就像我们看远处的东西，先眯起眼睛大概看一下轮廓。这一步会迅速减少数据量，并提取出最基础的边缘和纹理信息。”
        -   **输出：** 尺寸减半的特征图，通道数增加 (例如32通道)。
    -   **多级特征提取 (Stages - 例如后续的 Conv+C3 组合)：**
        -   “接下来，Backbone会经历多个‘阶段’。每个阶段通常包含：”
            -   一个 Conv 层（步长为2）：继续“压缩”特征图的空间尺寸（让模型看得更“远”，关注更大范围的上下文），同时增加通道数（让模型从更多“角度”去理解特征）。
            -   若干个 C3 模块：这是YOLOv5的核心‘特征加工厂’。C3模块借鉴了CSPNet的思想，结构比较精巧，它内部有很多小的卷积和Bottleneck（瓶颈）结构。它的好处是能在不大幅增加计算量的前提下，深度挖掘和提炼当前尺度的特征，并且让信息在网络中流动更顺畅，学习效率更高。”
        -   **特征的层级性：** “经过Backbone的层层处理，会得到不同‘深度’的特征图：”
            -   **P3 (例如Backbone中层4的C3输出):** 尺寸相对较大 (原图的 $1/8$)，通道数适中 (例如128)。它保留了较多的**空间细节和位置信息**，对检测小目标很有帮助。
            -   **P4 (例如Backbone中层6的C3输出):** 尺寸中等 (原图的 $1/16$)，通道数较多 (例如256)。它在细节和语义信息之间取得了平衡。
            -   **P5 (例如Backbone中层9的SPPF输出):** 尺寸最小 (原图的 $1/32$)，通道数最多 (例如512)。它包含了最丰富的**高级语义信息**（比如“这是一个物体的整体概念”），对检测大目标和理解场景上下文很有帮助。
    -   **SPPF模块 (在Backbone末端)：**
        -   “在Backbone的最后，通常会有一个SPPF模块。你可以把它想象成给模型装上了‘广角镜’和‘变焦镜’的组合。它通过不同大小的池化操作，让模型能同时关注到不同感受野的信息，把局部细节和全局上下文都‘看’到，这对于识别不同大小的物体非常有帮助。”
-   **对Backbone的初步理解和思考 (引出改进思路)：**
    -   “Backbone提取的特征质量直接决定了后续检测的上限。虽然YOLOv5s的Backbone已经很高效，但思考在提取这些多尺度特征的过程中，是否能让模型更‘智能’地判断哪些特征通道对特定的障碍物检测任务更重要呢？”

### 2. Neck (颈部网络 - FPN+PAN)：特征的“信息高速公路”与“融合中心”

-   **作用通俗讲：** “Backbone输出了好几份不同‘清晰度’和‘理解深度’的‘地图’（P3, P4, P5特征图）。Neck部分就像一个‘信息枢纽’，它的任务是把这些地图的优点结合起来，制作出几份‘超级地图’，既有高层地图的‘战略眼光’（语义信息），又有低层地图的‘精确导航’（位置信息）。”
-   **YOLOv5s (v7.0) Neck 结构解读：**
    -   **FPN (Feature Pyramid Network - 自顶向下)：**
        -   “首先，Neck会把Backbone最深层、语义最丰富的P5特征图，通过‘上采样’（Upsample，把小图放大）的方式，逐层和P4、P3特征图进行‘信息共享’（通过Concat拼接，然后用C3模块进一步融合处理）。”
        -   “这就好比，一个经验丰富的老侦察兵（P5）把他的‘大局观’传授给在前线观察细节的新兵（P3, P4），让新兵也能理解更宏观的模式。”
        -   **结果：** 生成了初步融合了高级语义信息的 P4_fused 和 P3_fused 特征图。
    -   **PAN (Path Aggregation Network - 自底向上)：**
        -   “仅仅从上往下传递信息还不够。PAN结构又增加了一条‘反向汇报’的路径。它会把刚刚在FPN中融合了语义信息的P3_fused特征图，通过‘下采样’（Conv，把大图缩小）的方式，再逐层和P4_fused、P5_fused（P5的融合版本）进行‘信息补充’。”
        -   “这就好比，前线新兵（P3_fused）把他们观察到的最新、最精确的‘地面情况’（定位信息）汇报给后方的指挥官（P4_fused, P5_fused），让指挥官的决策更接地气。”
        -   **结果：** 生成了最终用于检测的三个尺度的特征图：P3_detect_in, P4_detect_in, P5_detect_in。这三张图都充分融合了来自不同层级的语义和位置信息。
-   **对Neck的思考 (引出改进思路)：**
    -   “Neck是特征融合的关键环节。如果在这个阶段能进一步优化特征的表达，让模型更好地辨别和整合来自不同路径的有用信息，将直接惠及最终的检测头。”

### 3. Head (检测头 - Detect层)：最终的“目标锁定与识别”

-   **作用通俗讲：** “Head部分就是‘侦察兵’最终掏出‘望远镜和目标识别器’进行精确打击的部分。它接收来自Neck的三份‘超级地图’（P3, P4, P5的最终融合特征），然后在每张地图上进行预测。”
-   **YOLOv5s (v7.0) Head (Detect模块) 工作方式：**
    -   “对于Neck传来的每一张特征图（例如P3_detect_in），Detect模块会用一个 $1 \times 1$ 的小卷积（可以看作是一个小型的全连接层）将特征图的每个‘格子点’（grid cell）转换成预测信息。”
    -   **锚框 (Anchors) 的作用：** “在每个格子点上，模型会基于预设的几种不同形状和大小的‘锚框’（Anchors）来进行预测。这些锚框就像是预先画好的几个‘嫌疑框’。”
    -   **预测内容：** “对于每个锚框，模型会预测：”
        1.  **边界框调整：** 这个锚框需要向哪个方向移动多少、放大或缩小多少，才能正好框住物体 (与 $box\_loss$ 相关)。
        2.  **物体置信度：** 这个调整后的框里到底有没有要找的障碍物 (与 $obj\_loss$ 相关)。
        3.  **类别概率：** 如果有障碍物，它是定义的22个类别中的哪一个 (与 $cls\_loss$ 相关)。
    -   **多尺度预测：** “由于有P3, P4, P5三个尺度的输入，Detect层能在不同尺度上分别进行预测，这样就能同时检测到图片中的小、中、大各种尺寸的障碍物了。”
-   **NMS (非极大值抑制)：** “模型可能会对同一个物体产生多个重叠的预测框。NMS就像一个‘裁判’，它会根据置信度和重叠程度，把多余的框去掉，只保留最准的那个。” (这通常在Detect模块之后，在 `general.py` 中实现)

---
## 四、模型修改：给“侦察兵”装上更敏锐的“感官” (注意力机制)

基于对 YOLOv5s 工作原理的理解，团队的核心改进集中在引入和优化“注意力机制”。目标是让模型在复杂的视觉环境中能更有效地聚焦于关键信息，从而提升障碍物检测的性能。

### SEBlock (Squeeze-and-Excitation Block)

#### 1. 通俗易懂的角度 (SEBlock 如同特征图的“频道调音师”)

可以将特征图的每个通道想象成电视的一个频道。有些频道对当前想看的内容（例如，识别一个特定的障碍物）非常重要，信息量大；而另一些频道可能充满了噪音，或者与当前任务关系不大。

**SEBlock 的作用就像一个智能的“频道调音师”：**

-   **Squeeze (压缩 - “全局扫描”):** 首先，调音师会快速浏览所有频道（所有特征通道），了解每个频道大致在播放什么内容，并给出一个整体的“重要性”或“活跃度”评估。它不是看每个频道的细节，而是看每个频道的“整体信号强度”。
    -   在CNN中，这是通过**全局平均池化 (Global Average Pooling, GAP)** 实现的。它将每个二维的特征图（一个频道）压缩成一个单一的数值，这个数值代表了这个通道特征的全局分布信息。
-   **Excitation (激励 - “调节音量/权重”):** 根据全局扫描的结果，调音师现在知道了哪些频道可能更重要。然后，它会通过一个小型的“控制面板”（通常是两个全连接层）来学习如何根据这些全局信息，为每个频道分配一个“音量”或“权重”（0到1之间）。重要的频道，音量调高（权重接近1）；不重要的频道，音量调低（权重接近0）。
    -   在CNN中，这个“控制面板”通常由两个全连接层（FC layers）构成。第一个FC层降低维度（减少参数量和计算量，引入一个瓶颈结构），然后通过ReLU激活；第二个FC层再恢复到原来的通道数，并通过Sigmoid激活，将输出值限制在0到1之间，作为每个通道的权重。
-   **Scale (应用 - “应用调音结果”):** 最后，调音师将这些学习到的“音量”（权重）应用回原始的每个频道上。重要的特征通道被增强，不重要的特征通道被抑制。
    -   在CNN中，这是通过将学习到的通道权重（Excitation的输出）与原始特征图的对应通道进行**逐通道相乘 (Channel-wise Multiplication)** 来实现的。

**简单来说，SEBlock 让网络学会了“关注”那些对当前任务最有用的特征通道，并“忽略”那些不那么有用的通道，从而提升模型的表达能力。**

#### 2. 数学角度

SEBlock 的目标是显式地建模特征通道之间的相互依赖关系。它通过学习的方式自动获取每个特征通道的重要性，然后根据这个重要性去增强有用的特征并抑制用处不大的特征。

**结构：**

1.  **输入 (Input Feature Map):** $X \in \mathbb{R}^{H \times W \times C}$ (高 $\times$ 宽 $\times$ 通道数)
2.  **Squeeze 操作 (Global Information Embedding):**
    *   对输入的特征图 $X$ 在空间维度上进行全局平均池化 (GAP)。
    *   对于第 $k$ 个通道 $X_k \in \mathbb{R}^{H \times W}$，其 Squeeze 操作输出 $z_k$ 为：
        $$z_k = F_{sq}(X_k) = \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} X_k(i, j)$$
    *   这将生成一个通道描述符 $z \in \mathbb{R}^{1 \times 1 \times C}$。
3.  **Excitation 操作 (Adaptive Recalibration):**
    *   目的是捕获通道间的依赖关系，并生成一组通道权重 $s \in \mathbb{R}^{1 \times 1 \times C}$。
    *   它通常由两个全连接层 (FC) 和非线性激活函数组成：
        $$s = F_{ex}(z, W) = \sigma(W_2 \cdot \delta(W_1 \cdot z))$$
        *   $W_1 \in \mathbb{R}^{\frac{C}{r} \times C}$: 第一个FC层，将通道数从 $C$ 降到 $C/r$。$r$ 是缩减比率 (reduction ratio)。
        *   $\delta$: ReLU激活函数。
        *   $W_2 \in \mathbb{R}^{C \times \frac{C}{r}}$: 第二个FC层，将通道数从 $C/r$ 恢复到 $C$。
        *   $\sigma$: Sigmoid激活函数，将输出权重归一化到 $(0, 1)$ 范围内。
4.  **Scale 操作 (Reweighting):**
    *   将 Excitation 操作得到的通道权重 $s$ 应用于原始的输入特征图 $X$。
    *   对于第 $k$ 个通道，输出特征图 $\tilde{X}_k$ 为：
        $$\tilde{X}_k = F_{scale}(X_k, s_k) = s_k \cdot X_k$$
        这里 $s_k$ 是权重向量 $s$ 的第 $k$ 个元素。

**数学公式总结：**
$$z_c = \frac{1}{HW} \sum_{i=1}^{H} \sum_{j=1}^{W} u_c(i,j) \quad (\text{Squeeze})$$
$$s = \sigma(W_2 \cdot \text{ReLU}(W_1 \cdot z)) \quad (\text{Excitation})$$
$$\tilde{x}_c = s_c \cdot u_c \quad (\text{Scale})$$

#### 3. 在项目 (YOLOv5 障碍物检测) 中的放置策略

SEBlock 作为一种通道注意力机制，可以被插入到CNN的任何模块之后，以重新校准该模块输出的特征图。在YOLOv5这样的检测模型中，考虑的常见放置位置有：

-   **Backbone (骨干网络) 的关键模块之后：**
    -   例如，在每个 C3 模块之后（或者 Bottleneck 模块之后）。
    -   **原因：** Backbone 负责从原始图像中提取不同层次的特征。在早期层，SEBlock 可以帮助模型关注那些包含基础边缘、纹理信息的有用通道；在深层，可以帮助关注那些包含更抽象语义信息（如物体部件）的通道。这有助于 Backbone 提取出对障碍物识别更具判别力的特征。
    -   **实践：** 尝试了“在Backbone的每个C3模块之后加入SEBlock”。认为这是合理的，因为C3是YOLOv5 backbone中的核心特征提取单元。
-   **Neck (颈部网络，如 FPN, PAN) 的关键融合节点之后：**
    -   例如，在 FPN/PAN 结构中进行特征融合（如 Concat 后）的卷积层之后。
    -   **原因：** Neck 部分负责融合来自 Backbone 不同层级的特征，以生成对不同尺度目标都有效的特征图。在特征融合后加入 SEBlock，可以帮助模型更好地判断融合后的各个通道对于后续检测头的重要性，优化多尺度特征的表达。
    -   **实践：** 也尝试了“在Neck部分的关键特征融合节点（C3模块之后）加入SEBlock”。认为这也是合理的，因为Neck的输出直接供给检测头，优化这里的特征通道权重对最终检测性能有直接影响。

**为什么选择这么放置？**

-   **提升特征判别力：** 对于障碍物检测，不同的障碍物可能在不同的特征通道上表现出更强的响应。SEBlock 可以动态地增强这些响应强的通道，抑制噪声或不相关信息的通道，使得提取的特征对障碍物更敏感。
-   **适应性：** 障碍物的种类、形态、光照条件等都可能变化，SEBlock 允许网络根据输入自适应地调整通道权重，提高模型的鲁棒性。
-   **轻量级：** SEBlock 本身增加的参数量和计算量相对较小，尤其是当缩减比率 $r$ 设置得比较大时，使其可以方便地集成到现有网络中而不会带来过大的性能开销。

#### 4. SEBlock 的超参数调优

SEBlock 主要的超参数是**缩减比率 (reduction ratio) $r$**。

-   $r$ 控制了 Excitation 阶段第一个全连接层降维的程度 (通道数从 $C$ 降到 $C/r$)。
-   **较小的 $r$ (例如 $r=4, r=8$)：**
    -   中间层的通道数 $C/r$ 较大，模型有更大的容量去学习通道间的复杂关系。
    -   参数量和计算量相对较高，可能更容易过拟合。
-   **较大的 $r$ (例如 $r=16, r=32$)：**
    -   中间层的通道数 $C/r$ 较小，模型更轻量。
    -   对通道间关系的建模能力可能略有下降，但可以防止过拟合。
-   **如何选择 $r$？**
    -   **经验值：** 文献中常用的值是 $16$。
    -   **实验：** 这是确定最佳 $r$ 值的最可靠方法。在验证集上尝试了不同的 $r$ 值 (例如，$r=8, 16, 32$)，观察模型的性能指标 (如 mAP) 以及参数量/FLOPs 的变化，并权衡性能提升和模型复杂度的增加。
    -   **考虑原始通道数 $C$：** 如果 $C$ 本身就很小，过大的 $r$ 可能会过度压缩信息。
    -   **实践：** “对SEBlock的reduction_ratio ($r$) 参数进行了实验，例如$r=8, 16, 32$, 以观察不同压缩比对性能和模型复杂度的影响。” 会根据实验结果（例如mAP@0.5的变化）来决定哪个 $r$ 值在任务和数据集上表现最好。

---

### CBAM (Convolutional Block Attention Module)

#### 1. 通俗易懂的角度 (CBAM 如同特征图的“双重聚光灯操作员”)

可以将 CBAM 想象成一个经验丰富的“双重聚光灯操作员”，当在一个复杂的场景中寻找特定的物体（障碍物）时，它会从两个方面帮助聚焦：

1.  **频道聚光灯 (Channel Attention Module - CAM) - “哪些信息类别最重要？”**
    *   这部分和 SEBlock 的思路类似，但实现方式略有不同。操作员首先会判断，对于当前要找的物体，哪些“信息类别”（特征通道）是最相关的。
    *   CBAM 的 CAM 同时使用了**全局平均池化 (AvgPool)** 和**全局最大池化 (MaxPool)** 来聚合每个通道的空间信息。这两个信息被送到一个共享的小型多层感知机 (MLP) 中学习通道权重，然后将 MLP 的输出逐元素相加并通过 Sigmoid 得到最终的通道注意力权重。
    *   最后，这个通道权重被乘回到原始特征图上。
2.  **空间聚光灯 (Spatial Attention Module - SAM) - “图像的哪些区域最重要？”**
    *   在调整了信息类别的重要性之后，操作员现在要具体照亮图像中的关键区域。
    *   CBAM 的 SAM 在**通道维度**上进行操作。它首先对经过通道注意力加权后的特征图，在通道维度上分别进行**平均池化**和**最大池化**。
    *   然后将这两个结果**拼接 (Concatenate)** 起来，并通过一个标准的**卷积层 (通常是 $7 \times 7$ 大小的卷积核)** 来学习空间注意力图。
    *   最后，通过 Sigmoid 函数得到一个二维的空间注意力权重图，这个权重图被乘回到其输入特征图上。

**简单来说，CBAM 先告诉网络“应该关注哪些类型的特征信息（通道）”，然后再告诉网络“这些信息在图像的哪些位置（空间）更值得关注”。它是一种串联的、更全面的注意力机制。**

#### 2. 数学角度

CBAM 是一种轻量级且有效的注意力模块，它依次应用通道注意力和空间注意力，以自适应地调整特征。

**结构：**

给定一个中间特征图 $F \in \mathbb{R}^{H \times W \times C}$ 作为输入。

1.  **通道注意力模块 (Channel Attention Module - CAM):**
    *   **聚合空间信息：**
        $$F_{avg}^c = \text{AvgPool}(F)$$
        $$F_{max}^c = \text{MaxPool}(F)$$
    *   **共享 MLP 并合并激活：**
        $$M_c(F) = \sigma(\text{MLP}(F_{avg}^c) + \text{MLP}(F_{max}^c))$$
        其中 MLP 通常包含两个全连接层和一个缩减比率 $r$。
    *   **应用权重：**
        $$F' = M_c(F) \otimes F$$
        ($\otimes$ 表示逐元素乘法，会广播)
2.  **空间注意力模块 (Spatial Attention Module - SAM):**
    *   **聚合通道信息：**
        $$F_{avg}^s = \text{AvgPool}_{\text{channel}}(F')$$
        $$F_{max}^s = \text{MaxPool}_{\text{channel}}(F')$$
    *   **拼接与卷积：**
        $$M_s(F') = \sigma(f^{k \times k}([\text{Concat}(F_{avg}^s, F_{max}^s)]))$$
        其中 $f^{k \times k}$ 表示一个卷积核大小为 $k \times k$ (通常 $k=7$) 的卷积操作。
    *   **应用权重：**
        $$F'' = M_s(F') \otimes F'$$

**总结：CBAM 通过 $F \xrightarrow{\text{CAM}} F' \xrightarrow{\text{SAM}} F''$ 的流程进行特征提炼。**

#### 3. 在项目 (YOLOv5 障碍物检测) 中的放置策略

CBAM 同样可以灵活地放置在CNN的不同位置。由于它结合了通道和空间注意力，期望它能比单独的通道注意力带来更全面的性能提升。

-   **Backbone 的关键模块之后：** 与 SEBlock 类似，可以在 C3 模块或 Bottleneck 之后加入。
    -   **原因：** 认为这能够同时优化 Backbone 提取的特征在通道维度和空间维度上的表达。
-   **Neck 的关键融合节点之后：**
    -   **原因：** 在多尺度特征融合后，CBAM 可以帮助模型更好地从融合后的特征中筛选出与目标相关的通道和空间区域。
-   **实践：** “主要实验了将CBAM放置在Neck部分，替代之前SEBlock的位置，或者与SEBlock进行组合（如果时间允许）。”
    -   认为在 Neck 部分使用 CBAM 可能更有优势，因为 Neck 的输出直接影响检测结果，CBAM 的双重注意力机制有助于精确定位。

**为什么选择这么放置 (针对障碍物检测的优势)？**

-   **通道注意力 (来自CAM)：** 帮助区分不同类型的障碍物。
-   **空间注意力 (来自SAM)：** 帮助模型更精确地定位障碍物在图像中的位置，抑制背景干扰。
-   **互补性：** 通道和空间注意力是互补的。

#### 4. CBAM 的超参数调优

CBAM 的主要超参数包括：

1.  **通道注意力模块 (CAM) 中的缩减比率 $r$：**
    *   与 SEBlock 中的 $r$ 含义和调优方法类似。
    *   **实践：** YAML 中的 `ratio` 参数即为 $r$。
2.  **空间注意力模块 (SAM) 中的卷积核大小 `kernel_size`：**
    *   论文中推荐使用 $7 \times 7$。认为较大的卷积核有更大的感受野。
    *   **实践：** YAML 中的 `kernel_size` 参数。主要围绕 $k=7$ 进行实验，但也考虑了 $k=3$ 或 $k=5$ 的可能性。
3.  **`channels_from_yaml_arg0` (或类似参数 `in_channels`)：**
    *   这不是一个需要“调优”的超参数，而是模块的输入通道数，由前一层的输出决定。

**CBAM 的调优总结：**

-   主要关注 `ratio` ($r$) 和 `kernel_size`。
-   会通过实验，观察 mAP、参数量、FLOPs 等指标，结合实验结果来确定最佳配置。

---

### 3. YAML配置与模型构建的理解与实践

-   **工作与理解：**
    -   “为了实现这些网络结构的修改，深入学习了YOLOv5通过YAML配置文件来定义模型架构的方式。”
    -   **关键点：**
        -   理解了 `[from, number, module, args]` 这四个参数的精确含义和相互关系，特别是 `from` 索引在插入/删除模块后的正确更新。
        -   掌握了 `depth_multiple` 和 `width_multiple` 如何影响实际的网络深度和通道数。
        -   弄清了自定义模块（如SEBlock, CBAM）的 `__init__` 方法参数如何与YAML中 `args` 列表进行对应，特别是 `parse_model` 对已知模块和未知模块的参数传递方式的差异。
    -   **实践：**
        -   “编写并使用了 `test_model_build.py` 脚本来频繁验证修改后的YAML文件是否能够成功构建模型并通过前向传播，这帮助快速定位和修复了许多由于索引错误或参数不匹配导致的问题。”
        -   “还通过 `sys.path` 的修改，实现了在不‘污染’原始YOLOv5源码的情况下，让模型构建过程优先加载自定义的 `common.py` 文件，这保证了代码的整洁性和可管理性。”

---
## 六、训练过程与具身智能仿真模拟

在项目推进过程中，发现在真实硬件上进行开发遇到了很多问题，例如Realsense系列的SDK使用、机器人运动控制等等难度较高。因此，将目标转向先用 PyBullet 实现仿真模拟。同时，负责数据与训练的同学通过修改配置文件、重新训练，让自定义模型在原有的真实图像输入检测能力下降较少的条件下，提高了对 PyBullet 内置障碍物的检测能力。以下是一些模型在100个epoch训练下的数据对比。通过这些初步训练，能够筛选出具有提升潜力的模型进行进一步的训练，最终得到的权重将用于检测脚本。

训练工作基本结束后，开始同步推进检测脚本、决策逻辑脚本与仿真模拟脚本三项工作，并开始编写联调主程序。

联调成果是：可以实现小车在 PyBullet 环境内进行运动避障和实时环境检测。

---
## 七、项目复现 (写给读者)

本项目提供了两种复现方式：通过 Docker (推荐，环境一致性好) 或直接在本地环境搭建。

### 方式一：使用 Docker (推荐)

推荐使用 Docker，因为它可以确保在与开发时一致的环境中运行项目，避免因环境差异导致的问题。

1.  **环境准备:**
    *   **安装 Docker:** 请根据您的操作系统从 Docker 官网下载并安装 Docker Desktop (Windows/macOS) 或 Docker Engine (Linux)。
    *   **(可选，推荐用于GPU加速训练/推理) NVIDIA GPU 用户:**
        *   安装最新的 NVIDIA 驱动。
        *   安装 [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)，以便 Docker 容器可以使用 GPU。

2.  **获取项目文件:**
    *   下载本项目的压缩包并解压，或者通过 `git clone` 获取项目。

3.  **准备数据集 (如果需要重新训练或使用特定数据集进行评估/演示):**
    *   在您的项目根目录下创建一个用于存放数据集的文件夹，例如 `my_datasets/`。
    *   将您的数据集（例如，COCO格式的图片和标注文件，或已经转换为YOLO格式的数据）放入此文件夹。
    *   **注意:** 项目中使用的配置文件（如数据 `.yaml` 文件和模型 `.yaml` 文件）中的路径可能需要根据您实际挂载到容器内的数据集路径进行调整。推荐将宿主机的数据集目录挂载到容器内的 `/app/datasets`。

4.  **构建 Docker 镜像:**
    *   打开终端或命令行，导航到解压后的项目根目录 (包含 `Dockerfile` 文件的目录)。
    *   运行以下命令构建 Docker 镜像 (将 `your-custom-tag`替换为您想要的标签，如 `latest` 或 `v1.0`):
        ```bash
        docker build -t yolov5-tju-ai:your-custom-tag .
        ```
    *   构建过程可能需要一些时间，因为它会下载基础镜像并安装所有依赖。

5.  **运行 Docker 容器并执行任务:**

    推荐启动一个交互式的 Bash Shell，这样您可以在容器内灵活执行各种脚本。

    *   **启动交互式 Shell (通用，可根据需求添加 GPU 和 GUI 支持):**
        ```bash
        # 基础命令 (无 GPU, 无 GUI)
        docker run -it --rm \
            -v $(pwd)/my_datasets:/app/datasets \
            -v $(pwd)/runs_output:/app/yolov5/runs \
            yolov5-tju-ai:your-custom-tag \
            bash

        # Linux 用户，如果需要 PyBullet GUI (X11 转发):
        # xhost +local:docker
        # docker run -it --rm \
        #     --gpus all \
        #     --env="DISPLAY" \
        #     --volume="/tmp/.X11-unix:/tmp/.X11-unix:rw" \
        #     -v $(pwd)/my_datasets:/app/datasets \
        #     -v $(pwd)/runs_output:/app/yolov5/runs \
        #     yolov5-tju-ai:your-custom-tag \
        #     bash

        # macOS 用户 (需要 XQuartz 并配置, 如果需要 PyBullet GUI):
        # IP=$(ifconfig en0 | grep inet | awk '$1=="inet" {print $2}')
        # xhost + $IP
        # docker run -it --rm \
        #     --env DISPLAY=$IP:0 \
        #     -v /tmp/.X11-unix:/tmp/.X11-unix \
        #     -v $(pwd)/my_datasets:/app/datasets \
        #     -v $(pwd)/runs_output:/app/yolov5/runs \
        #     yolov5-tju-ai:your-custom-tag \
        #     bash

        # Windows 用户 (需要 VcXsrv 或 Xming 并配置, 如果需要 PyBullet GUI):
        # docker run -it --rm \
        #     --gpus all \
        #     -e DISPLAY=<您的Windows主机IP>:0.0 \
        #     -v ${PWD}/my_datasets:/app/datasets \
        #     -v ${PWD}/runs_output:/app/yolov5/runs \
        #     yolov5-tju-ai:your-custom-tag \
        #     bash
        ```
        *   **说明:**
            *   `-it`: 以交互模式运行并分配一个伪终端。
            *   `--rm`: 容器退出时自动删除。
            *   `-v $(pwd)/my_datasets:/app/datasets`: 将当前宿主机目录下的 `my_datasets` 文件夹挂载到容器内的 `/app/datasets` 路径。
            *   `-v $(pwd)/runs_output:/app/yolov5/runs`: 将当前宿主机目录下的 `runs_output` 文件夹挂载到容器内的 `/app/yolov5/runs` 路径，用于保存训练结果。
            *   `--gpus all`: (NVIDIA GPU 用户) 允许容器访问所有可用的 GPU。
            *   GUI 相关参数: 用于在容器内运行的图形化应用显示在宿主机上。

    *   **在容器内执行脚本:**
        成功进入容器后 (`root@<container_id>:/app#`)，您可以运行脚本：
        ```bash
        # (在容器内 /app 目录下)

        # 示例 1: 数据格式转换
        # python cocotoyolo.py --input_dir /app/datasets/raw_data --output_dir /app/datasets/yolo_data

        # 示例 2: 开始训练
        python yolov5/train.py \
            --img 640 \
            --batch 16 \
            --epochs 100 \
            --data /app/datasets/your_data_config.yaml \
            --cfg Code_from_CHI_Xu/yolov5s_your_custom_model.yaml \
            --weights yolov5s.pt \
            --project /app/yolov5/runs/train \
            --name my_docker_experiment

        # 示例 3: 验证模型
        python yolov5/val.py \
            --weights /app/yolov5/runs/train/my_docker_experiment/weights/best.pt \
            --data /app/datasets/your_data_config.yaml \
            --img 640 \
            --task test

        # 示例 4: 运行主仿真循环 (演示)
        python main_simulation_loop.py --weights /app/yolov5/runs/train/my_docker_experiment/weights/best.pt

        # exit # 退出容器
        ```

### 方式二：本地环境搭建 (不使用 Docker)

如果您希望直接在本地环境运行项目：

1.  **获取项目文件:**
    *   下载本项目的压缩包并解压，或者通过 `git clone` 获取项目。
2.  **创建 Python 虚拟环境 (推荐):**
    强烈建议使用虚拟环境 (如 venv 或 conda) 来隔离项目依赖。
    ```bash
    # 使用 venv (Python 3.9+ 推荐)
    # python -m venv venv_yolov5_tju
    # Windows: venv_yolov5_tju\Scripts\activate
    # Linux/macOS: source venv_yolov5_tju/bin/activate

    # 或使用 conda
    # conda create -n yolov5_tju_env python=3.9 -y
    # conda activate yolov5_tju_env
    ```
3.  **安装依赖:**
    进入项目根目录，安装 `requirements.txt` 中列出的依赖：
    ```bash
    pip install -r requirements.txt
    ```
    *   **注意:** `requirements.txt` 是为 Docker (Linux) 环境生成的。在 Windows 或 macOS 上，某些包的安装可能需要调整。
4.  **准备数据集和预训练权重:**
    *   确保数据集和预训练权重文件路径正确。
5.  **执行项目脚本:**
    *   **数据格式转换:** `python cocotoyolo.py`
    *   **开始训练:** `python yolov5/train.py ...` (参数同上)
    *   **验证模型:** `python yolov5/val.py ...` (参数同上)
    *   **运行主仿真循环 (演示):** `python main_simulation_loop.py ...` (参数同上)
    *   您应该能看到 PyBullet 界面和小车运动。

---

**重要提示给使用者：**

*   **路径配置：** 无论是使用 Docker 还是本地环境，请务必检查并根据您的实际情况修改项目中的数据和模型配置文件 (`.yaml`) 中的路径。
*   **权重文件：** 运行推理或仿真时，确保 `--weights` 参数指向正确的模型权重文件 (`.pt`)。
*   **GPU 支持：** 如果您希望使用 GPU 加速，请确保您的环境已正确配置。

---
## 八、遇到过的问题

1.  **基线模型表现过早良好：** 当时使用了预训练权重，且自定义数据集尚未完善，主要依赖官方数据集，导致模型在约35个epoch时表现已较好，掩盖了后续改进的真实难度。
2.  **三次自定义模型实验数据重合严重：** 负责训练的同学忘记修改配置文件路径，导致几次训练使用了相同的配置，得到了无效的重复结果。
3.  **部分自定义模型在100个epoch后表现不佳：** 分析原因有二：一方面是当时数据集的质量和多样性还不够高；另一方面是因为模型结构更改后，原有的预训练权重无法直接匹配和加载，导致模型只能从随机初始化开始训练，学习效率相对较低。
4.  **模型构建的权衡：** 最理想的方式其实是不对官方原生的 `common.py` 进行侵入式修改，以保持其原始性。但为了更直观和方便地集成自定义模块，并快速通过构建测试，选择了直接修改。
5.  **不同协作者的YOLOv5版本不同：** 通过 `git log` 查看版本信息，找到共同的 `commit hash` 进行同步，或者在紧急情况下直接复制某位成员的项目文件，以实现（最简单粗暴的）同步。
6.  **复杂冲突无法合并PR：** 遇到这种情况，没有捷径，只能耐心地逐行查看和修改冲突。
7.  **联调初期遇到的问题：** 大部分问题是由于不同脚本之间用到的工具（如YOLOv5相关库、PyBullet库）来源或版本不一致导致的。通过同步版本、详细核对数据接口解决了这些问题。

---
## 九、未来的目标

希望未来能将此项目应用于更实际的具身智能场景，例如辅助智能驾驶系统进行障碍物感知，或者从一个更简单的目标开始：为电控小车增加一个摄像头，利用训练的模型进行实时目标检测，实现智能避障功能。

---
## 十、结语

衷心感谢各位队友的努力与协作，感谢老师的指导，也感谢 Gemini 和 Deep Seek 在遇到难题时提供的帮助和启发。

在过去一周里，我自己与 Gemini 2.5 Pro 的对话量就达到了恐怖的150wtoken，聊崩了三个对话窗口；科技确实在改变生活和学习方式，根据 Gemini 的评估，如果没有 AI 的辅助，这个难度的期末大作业可能需要8到16周才能完成。而现在作为自动化专业的学生，能在相对较短的时间内掌握如此多的工具和技能，完整地经历技术选型、模型设计、训练优化到实际应用的全过程，这本身就是一件非常不容易也很有成就感的事。

谢谢大家！深度学习真的很好玩。

---
#### 附录：

每个人的代码贡献